\documentclass[a4paper,11pt]{article}
\title{Intelligent Multimedia Systems \\ Exam 2008}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\newcommand{\ds}{\displaystyle}
\newcommand{\tbf}{\textbf}

\begin{document}
	\maketitle
	\section*{Question 1: Indexing and searching}
	
	\subsection*{a}
	\tbf{Q.} What is a signature file and how does it work? \\
	\tbf{A.} A signature file is a data structure 
	
	\subsection*{b}
	\tbf{Q.} What is an inverted file and how does it work? \\
	\tbf{A.} Instances have a datastructure containing a mapping from content to the location in the database. When given a query, the inverted file (or index) will return all documents containing the query. For example, we can have blocks of texts containing words. A mapping to the words
	
	\subsection*{c} Certain terms carry more information than others. For example, nouns will have less frequencies among documents, but are more probable to satisfy the query than for example determiners which have many frequencies among documents. Low-frequency terms are more specific to satisfy the query, but when having multiple results for the same query, the document with the highest term-frequency will be the best choice.
	
	\subsection*{d}
	\tbf{Q.} How does relevance feedback work?
	\tbf{A.} Relevance feedback: user feedback on relevance of docs/images in initial set of results
	\begin{itemize}
		\item User issues a (short, simple) query
		\item The user marks returned documents/images as relevant or non- relevant.
		\item The system computes a better representation of the information need based on feedback.
		\item Relevance feedback can go through one or more iterations.
	\end{itemize}
	Idea: it may be difficult to formulate a good query when you don't know the collection well, so iterate

	\subsection*{e}
		Difference between positive and negative feedback is the class to which the user
		determines an instance belongs to. Thus, a relevant instance for the user results
		in positive feedback and a non-relevant instance  for the user results in negative
		feedback.

	\section*{Question 2: Retrieval Effectiveness}
		\subsection*{a}
			precision := (relevant found docs) / (total found docs)\\
			recall := (relevant found docs) / (total relevant docs)\\
			Check which docs are relevant, and count the number of found docs per query: in
			this case 4 for S1Q1, so S1's Q1 precision is 4/15 and S1's recall is 4/10. Repeat
			for S1Q2, S2Q1, S2Q2.
		\subsection*{b}
			Calculate precision for S1 at R=1/10, 2/10, 3/10 4/10 and plot.\\
			Calculate precision for S2 at R=1/10, 2/10, 3/10 4/10 and plot.\\
			Need to do this for both queries...

	\section*{Question 3: Object Colors}

	\section*{Question 4: Color Invariants}
	\subsection*{a}
		$\ds\frac{R}{R + G + B} = \ds\frac{I \cdot K_R \cos \theta}{I \cdot K_R \cos \theta + I \cdot K_G \cos \theta + I \cdot K_B \cos \theta} = \ds\frac{K_R}{K_R + K_G + K_B}$
	\subsection*{b}
		Flat surface, so surface normal is the same at $x_1$ and $x_2$. Thus it follows that
		$\ds\frac{R_{x_1} = I \cdot K_R(x_1) \cos \theta}{R_{x_2} = I \cdot K_R(x_2) \cos \theta} = \ds\frac{K_R(x_1)}{K_R(x_2)}$.

	\subsection*{c}

	\subsection*{d}
	\[\sigma_q = \sqrt{(2\cdot4^2)+(1\cdot4^2)}\] 
	is stable. It's not dependent on RGB. (derivatives of 2R+B wrt R or B are constants).
	\subsection*{e}
	\[\frac{d}{dG}=\frac{-1}{a^3}\cdot 2\]
	\[\sigma_q = \sqrt{(\frac{-2}{a^3}\cdot4^2)}\] 	
	\subsection*{f}
		$rgb$ becomes unstable when the intensity gets really low. When $r=\frac{R}{R+G+B}$ the nominator and denominator are near zero, the division will converge towards 1.
	\subsection*{g}
		Hue becomes unstable when the intensity and the saturation are near zero. This is because the arctan has an asymptote around the origin, therefore the hue will have big jumps when the intensity or the saturation.

\end{document}